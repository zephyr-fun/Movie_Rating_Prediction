{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ng training porcess...\n",
      "Epoch:0 not in Trainset:5 Batch:0 Loss:1.0876874923706055\n",
      "Epoch:0 not in Trainset:5 Batch:1 Loss:1.2643377780914307\n",
      "Epoch:0 not in Trainset:5 Batch:2 Loss:1.2082916498184204\n",
      "Epoch:0 not in Trainset:5 Batch:3 Loss:0.9853068590164185\n",
      "Epoch:0 not in Trainset:5 Batch:4 Loss:1.1323347091674805\n",
      "Epoch:0 not in Trainset:5 Batch:5 Loss:1.0332388877868652\n",
      "Epoch:0 not in Trainset:5 Batch:6 Loss:1.1793439388275146\n",
      "Epoch:0 not in Trainset:5 Batch:7 Loss:1.1427562236785889\n",
      "Epoch:0 not in Trainset:5 Batch:8 Loss:1.2442653179168701\n",
      "Epoch:0 not in Trainset:5 Batch:9 Loss:1.0652692317962646\n",
      "Epoch:0 not in Trainset:5 Batch:10 Loss:1.1116316318511963\n",
      "Epoch:0 not in Trainset:5 Batch:11 Loss:1.0882468223571777\n",
      "Epoch:0 not in Trainset:5 Batch:12 Loss:1.0796916484832764\n",
      "Epoch:0 not in Trainset:5 Batch:13 Loss:1.0803205966949463\n",
      "Epoch:0 not in Trainset:5 Batch:14 Loss:1.110840082168579\n",
      "Epoch:0 not in Trainset:5 Batch:15 Loss:1.0644032955169678\n",
      "Epoch:0 not in Trainset:5 Batch:16 Loss:0.8966169357299805\n",
      "Epoch:0 not in Trainset:5 Batch:17 Loss:0.9490631818771362\n",
      "Epoch:0 not in Trainset:5 Batch:18 Loss:1.0330734252929688\n",
      "Epoch:0 not in Trainset:5 Batch:19 Loss:1.0529595613479614\n",
      "Epoch:0 not in Trainset:5 Batch:20 Loss:1.0568729639053345\n",
      "Epoch:0 not in Trainset:5 Batch:21 Loss:1.1472716331481934\n",
      "Epoch:0 not in Trainset:5 Batch:22 Loss:1.0573533773422241\n",
      "Epoch:0 not in Trainset:5 Batch:23 Loss:1.19273042678833\n",
      "Epoch:0 not in Trainset:5 Batch:24 Loss:1.0051920413970947\n",
      "Epoch:0 not in Trainset:5 Batch:25 Loss:1.7199504375457764\n",
      "Epoch:0 not in Trainset:5 Batch:26 Loss:0.9579717516899109\n",
      "Epoch:0 not in Trainset:5 Batch:27 Loss:1.0095094442367554\n",
      "Epoch:0 not in Trainset:5 Batch:28 Loss:1.1394239664077759\n",
      "Epoch:0 not in Trainset:5 Batch:29 Loss:1.1526565551757812\n",
      "Epoch:0 not in Trainset:5 Batch:30 Loss:1.0253467559814453\n",
      "Epoch:0 not in Trainset:5 Batch:31 Loss:0.8958050012588501\n",
      "Epoch:0 not in Trainset:5 Batch:32 Loss:0.9767045974731445\n",
      "Epoch:0 not in Trainset:5 Batch:33 Loss:0.9849498271942139\n",
      "Epoch:0 not in Trainset:5 Batch:34 Loss:1.2135064601898193\n",
      "Epoch:0 not in Trainset:5 Batch:35 Loss:1.0629546642303467\n",
      "Epoch:0 not in Trainset:5 Batch:36 Loss:1.090721607208252\n",
      "Epoch:0 not in Trainset:5 Batch:37 Loss:1.01485276222229\n",
      "Epoch:0 not in Trainset:5 Batch:38 Loss:0.9851243495941162\n",
      "Epoch:0 not in Trainset:5 Batch:39 Loss:0.8116884231567383\n",
      "Epoch:0 not in Trainset:5 Batch:40 Loss:0.894483745098114\n",
      "Epoch:0 not in Trainset:5 Batch:41 Loss:1.054358959197998\n",
      "Epoch:0 not in Trainset:5 Batch:42 Loss:1.1624431610107422\n",
      "Epoch:0 not in Trainset:5 Batch:43 Loss:1.1697537899017334\n",
      "Epoch:0 not in Trainset:5 Batch:44 Loss:1.2692540884017944\n",
      "Epoch:0 not in Trainset:5 Batch:45 Loss:1.2217657566070557\n",
      "Epoch:0 not in Trainset:5 Batch:46 Loss:1.2246183156967163\n",
      "Epoch:0 not in Trainset:5 Batch:47 Loss:1.0473899841308594\n",
      "Epoch:0 not in Trainset:5 Batch:48 Loss:1.1259586811065674\n",
      "Epoch:0 not in Trainset:5 Batch:49 Loss:1.0219371318817139\n",
      "Epoch:0 not in Trainset:5 Batch:50 Loss:0.927980363368988\n",
      "Epoch:0 not in Trainset:5 Batch:51 Loss:0.9201337695121765\n",
      "Epoch:0 not in Trainset:5 Batch:52 Loss:0.8928714990615845\n",
      "Epoch:0 not in Trainset:5 Batch:53 Loss:1.1456875801086426\n",
      "Epoch:0 not in Trainset:5 Batch:54 Loss:1.1836353540420532\n",
      "Epoch:0 not in Trainset:5 Batch:55 Loss:1.0964196920394897\n",
      "Epoch:0 not in Trainset:5 Batch:56 Loss:1.032143473625183\n",
      "Epoch:0 not in Trainset:5 Batch:57 Loss:1.2303903102874756\n",
      "Epoch:0 not in Trainset:5 Batch:58 Loss:1.0339528322219849\n",
      "Epoch:0 not in Trainset:5 Batch:59 Loss:0.9684310555458069\n",
      "Epoch:0 not in Trainset:5 Batch:60 Loss:1.007100224494934\n",
      "Epoch:0 not in Trainset:5 Batch:61 Loss:1.3794324398040771\n",
      "Epoch:0 not in Trainset:5 Batch:62 Loss:0.8679589033126831\n",
      "Epoch:0 not in Trainset:5 Batch:63 Loss:1.0404276847839355\n",
      "Epoch:0 not in Trainset:5 Batch:64 Loss:1.0344898700714111\n",
      "Epoch:0 not in Trainset:5 Batch:65 Loss:1.2036912441253662\n",
      "Epoch:0 not in Trainset:5 Batch:66 Loss:1.1051523685455322\n",
      "Epoch:0 not in Trainset:5 Batch:67 Loss:0.9665679931640625\n",
      "Epoch:0 not in Trainset:5 Batch:68 Loss:1.1346511840820312\n",
      "Epoch:0 not in Trainset:5 Batch:69 Loss:1.208643913269043\n",
      "Epoch:0 not in Trainset:5 Batch:70 Loss:1.041130542755127\n",
      "Epoch:0 not in Trainset:5 Batch:71 Loss:0.8933812379837036\n",
      "Epoch:0 not in Trainset:5 Batch:72 Loss:1.0823781490325928\n",
      "Epoch:0 not in Trainset:5 Batch:73 Loss:0.9982287883758545\n",
      "Epoch:0 not in Trainset:5 Batch:74 Loss:1.0328495502471924\n",
      "Epoch:0 not in Trainset:5 Batch:75 Loss:0.9899824261665344\n",
      "Epoch:0 not in Trainset:5 Batch:76 Loss:0.9718173742294312\n",
      "Epoch:0 not in Trainset:5 Batch:77 Loss:1.1045947074890137\n",
      "Epoch:0 not in Trainset:5 Batch:78 Loss:1.1359379291534424\n",
      "Epoch:0 not in Trainset:5 Batch:79 Loss:1.378126859664917\n",
      "Epoch:0 not in Trainset:5 Batch:80 Loss:0.9252797365188599\n",
      "Epoch:0 not in Trainset:5 Batch:81 Loss:1.2455573081970215\n",
      "Epoch:0 not in Trainset:5 Batch:82 Loss:1.0374469757080078\n",
      "Epoch:0 not in Trainset:5 Batch:83 Loss:1.1416956186294556\n",
      "Epoch:0 not in Trainset:5 Batch:84 Loss:1.0871999263763428\n",
      "Epoch:0 not in Trainset:5 Batch:85 Loss:0.9460272789001465\n",
      "Epoch:0 not in Trainset:5 Batch:86 Loss:1.047728419303894\n",
      "Epoch:0 not in Trainset:5 Batch:87 Loss:1.254638671875\n",
      "Epoch:0 not in Trainset:5 Batch:88 Loss:1.161042332649231\n",
      "Epoch:0 not in Trainset:5 Batch:89 Loss:1.2732582092285156\n",
      "Epoch:0 not in Trainset:5 Batch:90 Loss:1.2618578672409058\n",
      "Epoch:0 not in Trainset:5 Batch:91 Loss:1.0057069063186646\n",
      "Epoch:0 not in Trainset:5 Batch:92 Loss:1.0210868120193481\n",
      "Epoch:0 not in Trainset:5 Batch:93 Loss:1.0747644901275635\n",
      "Epoch:0 not in Trainset:5 Batch:94 Loss:1.264371633529663\n",
      "Epoch:0 not in Trainset:5 Batch:95 Loss:1.1381704807281494\n",
      "Epoch:0 not in Trainset:5 Batch:96 Loss:1.0402169227600098\n",
      "Epoch:0 not in Trainset:5 Batch:97 Loss:1.096534013748169\n",
      "Epoch:0 not in Trainset:5 Batch:98 Loss:0.9571879506111145\n",
      "Epoch:0 not in Trainset:5 Batch:99 Loss:1.1024247407913208\n",
      "Epoch:0 not in Trainset:5 Batch:100 Loss:0.867177426815033\n",
      "Epoch:0 not in Trainset:5 Batch:101 Loss:1.013948678970337\n",
      "Epoch:0 not in Trainset:5 Batch:102 Loss:0.917468786239624\n",
      "Epoch:0 not in Trainset:5 Batch:103 Loss:1.1908519268035889\n",
      "Epoch:0 not in Trainset:5 Batch:104 Loss:0.8639565706253052\n",
      "Epoch:0 not in Trainset:5 Batch:105 Loss:1.0096805095672607\n",
      "Epoch:0 not in Trainset:5 Batch:106 Loss:1.1285862922668457\n",
      "Epoch:0 not in Trainset:5 Batch:107 Loss:1.111057996749878\n",
      "Epoch:0 not in Trainset:5 Batch:108 Loss:0.9936248064041138\n",
      "Epoch:0 not in Trainset:5 Batch:109 Loss:0.8615778684616089\n",
      "Epoch:0 not in Trainset:5 Batch:110 Loss:1.653194785118103\n",
      "Epoch:0 not in Trainset:5 Batch:111 Loss:0.9747651815414429\n",
      "Epoch:0 not in Trainset:5 Batch:112 Loss:0.8934146761894226\n",
      "Epoch:0 not in Trainset:5 Batch:113 Loss:1.1886284351348877\n",
      "Epoch:0 not in Trainset:5 Batch:114 Loss:0.9504179358482361\n",
      "Epoch:0 not in Trainset:5 Batch:115 Loss:0.9550633430480957\n",
      "Epoch:0 not in Trainset:5 Batch:116 Loss:1.1492738723754883\n",
      "Epoch:0 not in Trainset:5 Batch:117 Loss:1.003057837486267\n",
      "Epoch:0 not in Trainset:5 Batch:118 Loss:0.9913767576217651\n",
      "Epoch:0 not in Trainset:5 Batch:119 Loss:0.8534160256385803\n",
      "Epoch:0 not in Trainset:5 Batch:120 Loss:0.9969505667686462\n",
      "Epoch:0 not in Trainset:5 Batch:121 Loss:1.00412118434906\n",
      "Epoch:0 not in Trainset:5 Batch:122 Loss:1.1703786849975586\n",
      "Epoch:0 not in Trainset:5 Batch:123 Loss:1.3217594623565674\n",
      "Epoch:0 not in Trainset:5 Batch:124 Loss:1.221303939819336\n",
      "Epoch:0 not in Trainset:5 Batch:125 Loss:0.9787024259567261\n",
      "Epoch:0 not in Trainset:5 Batch:126 Loss:1.0300853252410889\n",
      "Epoch:0 not in Trainset:5 Batch:127 Loss:1.1787269115447998\n",
      "Epoch:0 not in Trainset:5 Batch:128 Loss:0.8393028974533081\n",
      "Epoch:0 not in Trainset:5 Batch:129 Loss:0.8293271660804749\n",
      "Epoch:0 not in Trainset:5 Batch:130 Loss:0.8032476305961609\n",
      "Epoch:0 not in Trainset:5 Batch:131 Loss:1.1564366817474365\n",
      "Epoch:0 not in Trainset:5 Batch:132 Loss:1.157343864440918\n",
      "Epoch:0 not in Trainset:5 Batch:133 Loss:1.135428786277771\n",
      "Epoch:0 not in Trainset:5 Batch:134 Loss:0.9333806037902832\n",
      "Epoch:0 not in Trainset:5 Batch:135 Loss:1.1135214567184448\n",
      "Epoch:0 not in Trainset:5 Batch:136 Loss:1.101672649383545\n",
      "Epoch:0 not in Trainset:5 Batch:137 Loss:0.9620239734649658\n",
      "Epoch:0 not in Trainset:5 Batch:138 Loss:1.0117632150650024\n",
      "Epoch:0 not in Trainset:5 Batch:139 Loss:1.389404296875\n",
      "Epoch:0 not in Trainset:5 Batch:140 Loss:0.9402170181274414\n",
      "Epoch:0 not in Trainset:5 Batch:141 Loss:0.9994632005691528\n",
      "Epoch:0 not in Trainset:5 Batch:142 Loss:0.98734450340271\n",
      "Epoch:0 not in Trainset:5 Batch:143 Loss:1.1866586208343506\n",
      "Epoch:0 not in Trainset:5 Batch:144 Loss:1.0445551872253418\n",
      "Epoch:0 not in Trainset:5 Batch:145 Loss:0.9393649697303772\n",
      "Epoch:0 not in Trainset:5 Batch:146 Loss:1.1883716583251953\n",
      "Epoch:0 not in Trainset:5 Batch:147 Loss:1.1181782484054565\n",
      "Epoch:0 not in Trainset:5 Batch:148 Loss:0.993445634841919\n",
      "Epoch:0 not in Trainset:5 Batch:149 Loss:0.8832787275314331\n",
      "Epoch:0 not in Trainset:5 Batch:150 Loss:1.0856480598449707\n",
      "Epoch:0 not in Trainset:5 Batch:151 Loss:0.8554189205169678\n",
      "Epoch:0 not in Trainset:5 Batch:152 Loss:1.0285704135894775\n",
      "Epoch:0 not in Trainset:5 Batch:153 Loss:0.991411030292511\n",
      "Epoch:0 not in Trainset:5 Batch:154 Loss:0.94022536277771\n",
      "Epoch:0 not in Trainset:5 Batch:155 Loss:1.1247378587722778\n",
      "Epoch:0 not in Trainset:5 Batch:156 Loss:1.1222515106201172\n",
      "Epoch:0 not in Trainset:5 Batch:157 Loss:1.0937459468841553\n",
      "Epoch:0 not in Trainset:5 Batch:158 Loss:1.3059827089309692\n",
      "Epoch:0 not in Trainset:5 Batch:159 Loss:1.1996347904205322\n",
      "Epoch:0 not in Trainset:5 Batch:160 Loss:1.1123578548431396\n",
      "Epoch:0 not in Trainset:5 Batch:161 Loss:1.0069860219955444\n",
      "Epoch:0 not in Trainset:5 Batch:162 Loss:1.0276730060577393\n",
      "Epoch:0 not in Trainset:5 Batch:163 Loss:0.9616880416870117\n",
      "Epoch:0 not in Trainset:5 Batch:164 Loss:0.8804502487182617\n",
      "Epoch:0 not in Trainset:5 Batch:165 Loss:0.9859179258346558\n",
      "Epoch:0 not in Trainset:5 Batch:166 Loss:1.121213436126709\n",
      "Epoch:0 not in Trainset:5 Batch:167 Loss:1.2203108072280884\n",
      "Epoch:0 not in Trainset:5 Batch:168 Loss:1.1455717086791992\n",
      "Epoch:0 not in Trainset:5 Batch:169 Loss:1.3059046268463135\n",
      "Epoch:0 not in Trainset:5 Batch:170 Loss:1.1837552785873413\n",
      "Epoch:0 not in Trainset:5 Batch:171 Loss:0.9460064172744751\n",
      "Epoch:0 not in Trainset:5 Batch:172 Loss:0.9122548699378967\n",
      "Epoch:0 not in Trainset:5 Batch:173 Loss:1.1264584064483643\n",
      "Epoch:0 not in Trainset:5 Batch:174 Loss:1.1739296913146973\n",
      "Epoch:0 not in Trainset:5 Batch:175 Loss:1.061339020729065\n",
      "Epoch:0 not in Trainset:5 Batch:176 Loss:1.1475657224655151\n",
      "Epoch:0 not in Trainset:5 Batch:177 Loss:1.0282156467437744\n",
      "Epoch:0 not in Trainset:5 Batch:178 Loss:1.1275482177734375\n",
      "Epoch:0 not in Trainset:5 Batch:179 Loss:0.7678694128990173\n",
      "Epoch:0 not in Trainset:5 Batch:180 Loss:1.341454267501831\n",
      "Epoch:0 not in Trainset:5 Batch:181 Loss:0.8280265927314758\n",
      "Epoch:0 not in Trainset:5 Batch:182 Loss:0.8938100337982178\n",
      "Epoch:0 not in Trainset:5 Batch:183 Loss:0.9988680481910706\n",
      "Epoch:0 not in Trainset:5 Batch:184 Loss:0.9295817017555237\n",
      "Epoch:0 not in Trainset:5 Batch:185 Loss:1.0759018659591675\n",
      "Epoch:0 not in Trainset:5 Batch:186 Loss:0.8269078135490417\n",
      "Epoch:0 not in Trainset:5 Batch:187 Loss:1.0869395732879639\n",
      "Epoch:0 not in Trainset:5 Batch:188 Loss:1.0859909057617188\n",
      "Epoch:0 not in Trainset:5 Batch:189 Loss:1.1061910390853882\n",
      "Epoch:0 not in Trainset:5 Batch:190 Loss:0.9474588632583618\n",
      "Epoch:0 not in Trainset:5 Batch:191 Loss:1.1033620834350586\n",
      "Epoch:0 not in Trainset:5 Batch:192 Loss:0.8824688792228699\n",
      "Epoch:0 not in Trainset:5 Batch:193 Loss:1.4709810018539429\n",
      "Epoch:0 not in Trainset:5 Batch:194 Loss:1.1706829071044922\n",
      "Epoch:0 not in Trainset:5 Batch:195 Loss:1.0396016836166382\n",
      "Epoch:0 not in Trainset:5 Batch:196 Loss:0.9365623593330383\n",
      "Epoch:0 not in Trainset:5 Batch:197 Loss:1.2142484188079834\n",
      "Epoch:0 not in Trainset:5 Batch:198 Loss:0.9734898805618286\n",
      "Epoch:0 not in Trainset:5 Batch:199 Loss:0.9317039251327515\n",
      "Epoch:0 not in Trainset:5 Batch:200 Loss:0.9597088098526001\n",
      "Epoch:0 not in Trainset:5 Batch:201 Loss:1.153914451599121\n",
      "Epoch:0 not in Trainset:5 Batch:202 Loss:1.1014580726623535\n",
      "Epoch:0 not in Trainset:5 Batch:203 Loss:1.1362659931182861\n",
      "Epoch:0 not in Trainset:5 Batch:204 Loss:1.0226798057556152\n",
      "Epoch:0 not in Trainset:5 Batch:205 Loss:0.732467770576477\n",
      "Epoch:0 not in Trainset:5 Batch:206 Loss:1.043013572692871\n",
      "Epoch:0 not in Trainset:5 Batch:207 Loss:1.098382592201233\n",
      "Epoch:0 not in Trainset:5 Batch:208 Loss:1.2065412998199463\n",
      "Epoch:0 not in Trainset:5 Batch:209 Loss:1.233719825744629\n",
      "Epoch:0 not in Trainset:5 Batch:210 Loss:0.9552128314971924\n",
      "Epoch:0 not in Trainset:5 Batch:211 Loss:0.9809521436691284\n",
      "Epoch:0 not in Trainset:5 Batch:212 Loss:0.8429977893829346\n",
      "Epoch:0 not in Trainset:5 Batch:213 Loss:0.9467129111289978\n",
      "Epoch:0 not in Trainset:5 Batch:214 Loss:1.1916252374649048\n",
      "Epoch:0 not in Trainset:5 Batch:215 Loss:1.051574945449829\n",
      "Epoch:0 not in Trainset:5 Batch:216 Loss:1.0661518573760986\n",
      "Epoch:0 not in Trainset:5 Batch:217 Loss:0.9605989456176758\n",
      "Epoch:0 not in Trainset:5 Batch:218 Loss:1.14927077293396\n",
      "Epoch:0 not in Trainset:5 Batch:219 Loss:1.1672770977020264\n",
      "Epoch:0 not in Trainset:5 Batch:220 Loss:0.9747031927108765\n",
      "Epoch:0 not in Trainset:5 Batch:221 Loss:1.0438941717147827\n",
      "Epoch:0 not in Trainset:5 Batch:222 Loss:1.0804781913757324\n",
      "Epoch:0 not in Trainset:5 Batch:223 Loss:1.0170166492462158\n",
      "Epoch:0 not in Trainset:5 Batch:224 Loss:1.1243188381195068\n",
      "Epoch:0 not in Trainset:5 Batch:225 Loss:1.0289006233215332\n",
      "Epoch:0 not in Trainset:5 Batch:226 Loss:1.090542197227478\n",
      "Epoch:0 not in Trainset:5 Batch:227 Loss:0.8962358236312866\n",
      "Epoch:0 not in Trainset:5 Batch:228 Loss:1.035507082939148\n",
      "Epoch:0 not in Trainset:5 Batch:229 Loss:0.7953120470046997\n",
      "Epoch:0 not in Trainset:5 Batch:230 Loss:1.0489578247070312\n",
      "Epoch:0 not in Trainset:5 Batch:231 Loss:0.9071099758148193\n",
      "Epoch:0 not in Trainset:5 Batch:232 Loss:1.0252752304077148\n",
      "Epoch:0 not in Trainset:5 Batch:233 Loss:1.0724929571151733\n",
      "Epoch:0 not in Trainset:5 Batch:234 Loss:1.125474214553833\n",
      "Epoch:0 not in Trainset:5 Batch:235 Loss:1.0110633373260498\n",
      "Epoch:0 not in Trainset:5 Batch:236 Loss:1.340092420578003\n",
      "Epoch:0 not in Trainset:5 Batch:237 Loss:1.100189447402954\n",
      "Epoch:0 not in Trainset:5 Batch:238 Loss:1.2107040882110596\n",
      "Epoch:0 not in Trainset:5 Batch:239 Loss:1.0825713872909546\n",
      "Epoch:0 not in Trainset:5 Batch:240 Loss:0.9879323244094849\n",
      "Epoch:0 not in Trainset:5 Batch:241 Loss:1.134284496307373\n",
      "Epoch:0 not in Trainset:5 Batch:242 Loss:0.9077799320220947\n",
      "Epoch:0 not in Trainset:5 Batch:243 Loss:0.9451838135719299\n",
      "Epoch:0 not in Trainset:5 Batch:244 Loss:0.9962682723999023\n",
      "Epoch:0 not in Trainset:5 Batch:245 Loss:1.1229782104492188\n",
      "Epoch:0 not in Trainset:5 Batch:246 Loss:1.137049674987793\n",
      "Epoch:0 not in Trainset:5 Batch:247 Loss:1.190325140953064\n",
      "Epoch:0 not in Trainset:5 Batch:248 Loss:1.4166431427001953\n",
      "Epoch:0 not in Trainset:5 Batch:249 Loss:1.181044340133667\n",
      "Epoch:0 not in Trainset:5 Batch:250 Loss:0.9793490171432495\n",
      "Epoch:0 not in Trainset:5 Batch:251 Loss:0.9084262847900391\n",
      "Epoch:0 not in Trainset:5 Batch:252 Loss:1.1440768241882324\n",
      "Epoch:0 not in Trainset:5 Batch:253 Loss:1.132652759552002\n",
      "Epoch:0 not in Trainset:5 Batch:254 Loss:1.1024219989776611\n",
      "Epoch:0 not in Trainset:5 Batch:255 Loss:1.081786870956421\n",
      "Epoch:0 not in Trainset:5 Batch:256 Loss:1.0808180570602417\n",
      "Epoch:0 not in Trainset:5 Batch:257 Loss:0.9092341661453247\n",
      "Epoch:0 not in Trainset:5 Batch:258 Loss:1.0723223686218262\n",
      "Epoch:0 not in Trainset:5 Batch:259 Loss:1.1327402591705322\n",
      "Epoch:0 not in Trainset:5 Batch:260 Loss:1.1185741424560547\n",
      "Epoch:0 not in Trainset:5 Batch:261 Loss:0.8469886779785156\n",
      "Epoch:0 not in Trainset:5 Batch:262 Loss:0.8751737475395203\n",
      "Epoch:0 not in Trainset:5 Batch:263 Loss:0.9639193415641785\n",
      "Epoch:0 not in Trainset:5 Batch:264 Loss:1.0426158905029297\n",
      "Epoch:0 not in Trainset:5 Batch:265 Loss:0.9677761197090149\n",
      "Epoch:0 not in Trainset:5 Batch:266 Loss:0.7893253564834595\n",
      "Epoch:0 not in Trainset:5 Batch:267 Loss:1.1963229179382324\n",
      "Epoch:0 not in Trainset:5 Batch:268 Loss:1.0934407711029053\n",
      "Epoch:0 not in Trainset:5 Batch:269 Loss:1.1278643608093262\n",
      "Epoch:0 not in Trainset:5 Batch:270 Loss:1.1237831115722656\n",
      "Epoch:0 not in Trainset:5 Batch:271 Loss:1.0326306819915771\n",
      "Epoch:0 not in Trainset:5 Batch:272 Loss:1.0057899951934814\n",
      "Epoch:0 not in Trainset:5 Batch:273 Loss:0.8951085805892944\n",
      "Epoch:0 not in Trainset:5 Batch:274 Loss:1.8629863262176514\n",
      "Epoch:0 not in Trainset:5 Batch:275 Loss:0.9930095672607422\n",
      "Epoch:0 not in Trainset:5 Batch:276 Loss:1.0433017015457153\n",
      "Epoch:0 not in Trainset:5 Batch:277 Loss:0.9198421239852905\n",
      "Epoch:0 not in Trainset:5 Batch:278 Loss:1.2095832824707031\n",
      "Epoch:0 not in Trainset:5 Batch:279 Loss:0.9912031888961792\n",
      "Epoch:0 not in Trainset:5 Batch:280 Loss:0.9270756840705872\n",
      "Epoch:0 not in Trainset:5 Batch:281 Loss:0.9699422121047974\n",
      "Epoch:0 not in Trainset:5 Batch:282 Loss:1.0462970733642578\n",
      "Epoch:0 not in Trainset:5 Batch:283 Loss:1.175837516784668\n",
      "Epoch:0 not in Trainset:5 Batch:284 Loss:1.1121249198913574\n",
      "Epoch:0 not in Trainset:5 Batch:285 Loss:1.0550795793533325\n",
      "Epoch:0 not in Trainset:5 Batch:286 Loss:1.0012404918670654\n",
      "Epoch:0 not in Trainset:5 Batch:287 Loss:0.7947357892990112\n",
      "Epoch:0 not in Trainset:5 Batch:288 Loss:1.1356384754180908\n",
      "Epoch:0 not in Trainset:5 Batch:289 Loss:1.1700456142425537\n",
      "Epoch:0 not in Trainset:5 Batch:290 Loss:1.195009708404541\n",
      "Epoch:0 not in Trainset:5 Batch:291 Loss:1.1502047777175903\n",
      "Epoch:0 not in Trainset:5 Batch:292 Loss:1.2607407569885254\n",
      "Epoch:0 not in Trainset:5 Batch:293 Loss:1.007013201713562\n",
      "Epoch:0 not in Trainset:5 Batch:294 Loss:1.1578187942504883\n",
      "Epoch:0 not in Trainset:5 Batch:295 Loss:0.8593851327896118\n",
      "Epoch:0 not in Trainset:5 Batch:296 Loss:0.8146113157272339\n",
      "Epoch:0 not in Trainset:5 Batch:297 Loss:1.2006826400756836\n",
      "Epoch:0 not in Trainset:5 Batch:298 Loss:1.101828694343567\n",
      "Epoch:0 not in Trainset:5 Batch:299 Loss:1.0164167881011963\n",
      "Epoch:0 not in Trainset:5 Batch:300 Loss:1.1268954277038574\n",
      "Epoch:0 not in Trainset:5 Batch:301 Loss:0.939355731010437\n",
      "Epoch:0 not in Trainset:5 Batch:302 Loss:1.3346357345581055\n",
      "Epoch:0 not in Trainset:5 Batch:303 Loss:0.9014032483100891\n",
      "Epoch:0 not in Trainset:5 Batch:304 Loss:1.0426353216171265\n",
      "Epoch:0 not in Trainset:5 Batch:305 Loss:1.0871214866638184\n",
      "Epoch:0 not in Trainset:5 Batch:306 Loss:1.055177092552185\n",
      "Epoch:0 not in Trainset:5 Batch:307 Loss:1.0923537015914917\n",
      "Epoch:0 not in Trainset:5 Batch:308 Loss:0.9903947114944458\n",
      "Epoch:0 not in Trainset:5 Batch:309 Loss:0.8918709754943848\n",
      "Epoch:0 not in Trainset:5 Batch:310 Loss:1.0307796001434326\n",
      "Epoch:0 not in Trainset:5 Batch:311 Loss:0.9714900851249695\n",
      "Epoch:0 not in Trainset:5 Batch:312 Loss:1.174710988998413\n",
      "----------------\n",
      "Starting testing porcess...\n",
      "Epoch:0 Testset:5 Batch:0 Loss:1.097939834785184\n",
      "Epoch:0 Testset:5 Batch:1 Loss:1.1335114535772308\n",
      "Epoch:0 Testset:5 Batch:2 Loss:1.0816986587435493\n",
      "Epoch:0 Testset:5 Batch:3 Loss:1.010162933871729\n",
      "Epoch:0 Testset:5 Batch:4 Loss:1.0182579135969005\n",
      "Epoch:0 Testset:5 Batch:5 Loss:0.9666412543274083\n",
      "Epoch:0 Testset:5 Batch:6 Loss:1.1494306428084102\n",
      "Epoch:0 Testset:5 Batch:7 Loss:0.9375397994484598\n",
      "Epoch:0 Testset:5 Batch:8 Loss:0.9507995046041675\n",
      "Epoch:0 Testset:5 Batch:9 Loss:1.0407217378118534\n",
      "Epoch:0 Testset:5 Batch:10 Loss:1.038577329262694\n",
      "Epoch:0 Testset:5 Batch:11 Loss:0.9227572774819733\n",
      "Epoch:0 Testset:5 Batch:12 Loss:0.8763469724514014\n",
      "Epoch:0 Testset:5 Batch:13 Loss:0.9032926117439026\n",
      "Epoch:0 Testset:5 Batch:14 Loss:1.0165297767868609\n",
      "Epoch:0 Testset:5 Batch:15 Loss:0.8362594991871786\n",
      "Epoch:0 Testset:5 Batch:16 Loss:0.8285499811381032\n",
      "Epoch:0 Testset:5 Batch:17 Loss:0.8912379092190702\n",
      "Epoch:0 Testset:5 Batch:18 Loss:0.9037648569610484\n",
      "Epoch:0 Testset:5 Batch:19 Loss:1.0954989873214946\n",
      "Epoch:0 MSE on cv_5_df:0.984975946756431 Time:2021-01-04 15:26:13\n",
      "Epoch:0 Average MSE:1.0764921840689035 Time:2021-01-04 15:26:13\n",
      "torch save model successfully to Deep_Model/Deep_Model_Weights.bin\n",
      "torch_jit save model successfully to Deep_Model/Deep_Model.pt\n",
      "torch_onnx save model successfully to Deep_Model/Deep_Model_ONNX.onx\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'usr_num', 'item_num', 'emb_usr_size', 'emb_item_size', and 'hidden_size'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4e88d9c892c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mDeepModel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRecModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRecModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'E:\\信息检索与文本挖掘\\范宣哲\\MovieRatingPrediction.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 5 required positional arguments: 'usr_num', 'item_num', 'emb_usr_size', 'emb_item_size', and 'hidden_size'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorwatch as tw\n",
    "from DeepModel import RecModel\n",
    "\n",
    "model=RecModel(943,1682,50,150,25)\n",
    "img = tw.draw_model(model, [1024,1024])\n",
    "img.save(r'E:\\信息检索与文本挖掘\\范宣哲\\MovieRatingPrediction.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}